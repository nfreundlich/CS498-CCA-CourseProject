{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import datetime\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "AWS_BUCKET_NAME = '1-cca-ted-raw-dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function download_files:\n",
    "# FTPs to ftp_path, gets list of files in the directory for the current year and month (note that this may cause\n",
    "# problems on the first day of the month downloading the files from the last day of the previous month); makes a list \n",
    "# of files to be download. Then downloads the files with wget (ftp was not downloading the entire file); unzips the \n",
    "# tarballs and then deletes the tar.gz file.\n",
    "# \n",
    "# Params:\n",
    "# - data_path -> path to download data to\n",
    "# - ftp_path -> URI for FTP\n",
    "# - username, password -> username and password for FTP login\n",
    "# - year, month -> year and month to download data for, if None will use current month and year\n",
    "# - max_files -> max number of files to download, useful for debugging\n",
    "# - delete_files -> whether to delete the files that have been successfully extracted\n",
    "#\n",
    "# Returns:\n",
    "# - list of files downloaded and successfully extracted\n",
    "#\n",
    "# Note that sometimes using the URL throws an error, in this case use the IP address: 91.250.107.123\n",
    "\n",
    "# Function download_files:\n",
    "# FTPs to ftp_path, gets list of files in the directory for the current year and month (note that this may cause\n",
    "# problems on the first day of the month downloading the files from the last day of the previous month); makes a list \n",
    "# of files to be download. Then downloads the files with wget (ftp was not downloading the entire file); unzips the \n",
    "# tarballs and then deletes the tar.gz file.\n",
    "# \n",
    "# Params:\n",
    "# - data_path -> path to download data to\n",
    "# - ftp_path -> URI for FTP\n",
    "# - username, password -> username and password for FTP login\n",
    "# - year, month -> year and month to download data for, if None will use current month and year\n",
    "# - max_files -> max number of files to download, useful for debuggin\n",
    "# - delete_files -> whether to delete the files that have been successfully extracted\n",
    "#\n",
    "# Returns:\n",
    "# - list of files downloaded and successfully extracted\n",
    "#\n",
    "# Note that sometimes using the URL throws an error, in this case use the IP address: 91.250.107.123\n",
    "\n",
    "def download_files(data_path=\"/tmp\", ftp_path=\"91.250.107.123\", username=\"guest\", password=\"guest\", year=None, month=None, max_files=1, delete_files=True):\n",
    "    ## USE FTP TO GET THE LIST OF FILES TO DOWNLOAD\n",
    "    with FTP(ftp_path, user=username, passwd=password) as ftp:\n",
    "        # create the directory name for the current month and year\n",
    "        # we may want to do this for yesterday \n",
    "        now = datetime.datetime.now()\n",
    "        if year is None:\n",
    "            year = now.year\n",
    "        if month is None:\n",
    "            month = now.month\n",
    "            if now.day == 1:\n",
    "                month -= 1\n",
    "                if month == 0:\n",
    "                    month = 12\n",
    "                    year -= 1\n",
    "        \n",
    "        month = str(month).zfill(2)\n",
    "        year = str(year)\n",
    "         \n",
    "        # go to that directory and get the files in it\n",
    "        ftp.cwd('daily-packages/' + year + \"/\" + month) \n",
    "        dir_list = ftp.nlst() \n",
    "        files_to_download = []\n",
    "\n",
    "        # loop through the files\n",
    "        for file in dir_list:\n",
    "            # download the file with wget since ftplib seems to only download a small part of the file\n",
    "            file_path = \"ftp://\"+username+\":\"+password+\"@\" + ftp_path + \"/daily-packages/\" + year + \"/\" + month + \"/\" + file\n",
    "            files_to_download.append(file_path)\n",
    "    \n",
    "    # the newest file will be the last one in the sorted list\n",
    "    if max_files is not None:\n",
    "        files_to_download = sorted(files_to_download)[-max_files:]\n",
    "    \n",
    "    # download the files with wget so we can download the entire file without errors            \n",
    "    downloaded_files = []\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            print(\"\\nDownloading\", file)\n",
    "            file_name = file.split(\"/\")[-1]\n",
    "            d_file = urllib.request.urlretrieve(file, os.path.join(data_path, file_name))[0]\n",
    "            downloaded_files.append(d_file)\n",
    "        except Exception as e:\n",
    "            print(\"Error downloading\", file, e)\n",
    "            \n",
    "    return downloaded_files\n",
    "\n",
    "def upload_to_s3(data_path=\"/tmp\", key=\"raw_data\"):\n",
    "    bucket = s3.Bucket(AWS_BUCKET_NAME)\n",
    "    \n",
    "    # find the directories in the download dir\n",
    "    files = os.listdir(data_path)\n",
    "    for file in files:\n",
    "        print(\"Uploading\", file)\n",
    "        try:\n",
    "            s3.meta.client.upload_file(Filename = os.path.join(data_path, file), Bucket = AWS_BUCKET_NAME, Key = key + \"/\" + file)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181117_2018222.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181120_2018223.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181121_2018224.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181122_2018225.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181123_2018226.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181124_2018227.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181127_2018228.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181128_2018229.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181129_2018230.tar.gz\n",
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/11/20181130_2018231.tar.gz\n"
     ]
    }
   ],
   "source": [
    "new_files = download_files(data_path=\"tmp\", max_files=10, month=11, year=2018, delete_files=True)\n",
    "    \n",
    "# upload_to_s3(data_path=\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(data_path=\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
