{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions which will download the tar.gz files, extract them, log the files downloaded so as not to redownload them in the future. Then another function is passed the names of the files successfully extracted and that function loops through the files, loads and parses them - also logging the names of the .xml files successfully processed.\n",
    "\n",
    "Note that this depends on the scheme TED uses to name the folders inside of the .tar.gz files not changing (the folder has a different name than the containing tarball.) There are a couple possible solutions to this I can think of:\n",
    " * Download the files to a temp directory and move them after they have been processed.\n",
    " * Delete the .xml files from disk after having been processed (or they can be put into a NoSQL database) - they can always be redownloaded if necessary.\n",
    " \n",
    "The next steps will be determing which of the 1000+ columns to keep and which to drop and putting the data into a database (we need to choose a database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import datetime\n",
    "import os\n",
    "import tarfile\n",
    "import wget\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "import io\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180124_2018016.tar.gz\"\n",
    "\n",
    "d_file = urllib.request.urlretrieve(url, 'foo.tar.gz')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foo.tar.gz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"logs\"\n",
    "TMP_PATH = \"tmp\"\n",
    "\n",
    "# load the set of columns we will use\n",
    "USE_COLS = np.load(\"columns.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function download_files:\n",
    "# FTPs to ftp_path, gets list of files in the directory for the current year and month (note that this may cause\n",
    "# problems on the first day of the month downloading the files from the last day of the previous month); makes a list \n",
    "# of files to be download. Then downloads the files with wget (ftp was not downloading the entire file); unzips the \n",
    "# tarballs and then deletes the tar.gz file.\n",
    "# \n",
    "# Params:\n",
    "# - data_path -> path to download data to\n",
    "# - ftp_path -> URI for FTP\n",
    "# - username, password -> username and password for FTP login\n",
    "# - year, month -> year and month to download data for, if None will use current month and year\n",
    "# - max_files -> max number of files to download, useful for debuggin\n",
    "# - delete_files -> whether to delete the files that have been successfully extracted\n",
    "#\n",
    "# Returns:\n",
    "# - list of files downloaded and successfully extracted\n",
    "#\n",
    "# Note that sometimes using the URL throws an error, in this case use the IP address: 91.250.107.123\n",
    "\n",
    "def download_files(data_path=\"data\", ftp_path=\"91.250.107.123\", username=\"guest\", password=\"guest\", year=None, month=None, max_files=None, delete_files=True):\n",
    "    # open the log of downloaded files so we know what NOT to download\n",
    "    downloaded_files = pd.read_csv(os.path.join(LOG_PATH, \"download_logs.txt\"), header=None).values\n",
    "    \n",
    "    ## USE FTP TO GET THE LIST OF FILES TO DOWNLOAD\n",
    "    with FTP(ftp_path, user=username, passwd=password) as ftp:\n",
    "        # create the directory name for the current month and year\n",
    "        # we may want to do this for yesterday \n",
    "        now = datetime.datetime.now()\n",
    "        if year is None:\n",
    "            year = str(now.year)\n",
    "        if month is None:\n",
    "            month = datetime.datetime.now().strftime('%m')\n",
    "\n",
    "        # go to that directory and get the files in it\n",
    "        ftp.cwd('daily-packages/' + year + \"/\" + month) \n",
    "        dir_list = ftp.nlst() \n",
    "        files_to_download = []\n",
    "\n",
    "        # loop through the files\n",
    "        for file in dir_list:\n",
    "            # if the file is not in the logs\n",
    "            if file not in downloaded_files:\n",
    "                # download the file with wget since ftplib seems to only download a small part of the file\n",
    "                file_path = \"ftp://\"+username+\":\"+password+\"@\" + ftp_path + \"/daily-packages/\" + year + \"/\" + month + \"/\" + file\n",
    "                files_to_download.append(file_path)\n",
    "    \n",
    "    # delete the downloaded files\n",
    "    del(downloaded_files)\n",
    "    \n",
    "    if max_files is not None:\n",
    "        files_to_download = files_to_download[:max_files]\n",
    "    \n",
    "    # download the files with wget so we can download the entire file without errors            \n",
    "    downloaded_files = []\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            print(\"\\nDownloading\", file)\n",
    "            file_name = file.split(\"/\")[-1]\n",
    "#             d_file = wget.download(file, data_path)\n",
    "            d_file = urllib.request.urlretrieve(file, os.path.join(data_path, file_name))\n",
    "            downloaded_files.append(d_file)\n",
    "        except:\n",
    "            print(\"Error downloading\", file)\n",
    "    \n",
    "    extracted_files = []\n",
    "    # extract the tarballs\n",
    "    for file in downloaded_files:\n",
    "        print(\"\\nExtracting:\", file)\n",
    "        try:\n",
    "            if (file.endswith(\"tar.gz\")):\n",
    "                tar = tarfile.open(file, \"r:gz\")\n",
    "                tar.extractall(data_path)\n",
    "                tar.close()\n",
    "            elif (file.endswith(\"tar\")):\n",
    "                tar = tarfile.open(file, \"r:\")\n",
    "                tar.extractall()\n",
    "                tar.close()\n",
    "            \n",
    "            extracted_files.append(file)\n",
    "            if delete_files:\n",
    "                # if everything was properly extracted we can delete the file\n",
    "                os.remove(file)\n",
    "            \n",
    "            # try to open the file in append mode, if it doesn't work create a new file\n",
    "            try:\n",
    "                f = open(os.path.join(LOG_PATH, \"download_logs.txt\"),\"a\")\n",
    "            except:\n",
    "                f = open(os.path.join(LOG_PATH, \"download_logs.txt\"),\"w+\")\n",
    "            file_name = file.split(\"/\")[1]\n",
    "            f.write(file_name + \"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "        except:\n",
    "            print(\"Error extracting\", file)\n",
    "\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all currencies to EUR\n",
    "def convert_currencies(values, currencies):\n",
    "    url = \"https://api.exchangeratesapi.io/latest\"\n",
    "    content = urllib.request.urlopen(url).read()\n",
    "    exchange_rates = json.loads(content.decode())\n",
    "    results = []\n",
    "    \n",
    "    for value, currency in zip(values, currencies):\n",
    "        if currency == \"EUR\":\n",
    "            results.append(value)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                exchange_rate = exchange_rates['rates'][currency]\n",
    "                converted_value = float(value) / exchange_rate\n",
    "                results.append(converted_value)\n",
    "            # if we don't have a rate for the currency use NaN\n",
    "            except:\n",
    "                results.append(np.nan)\n",
    "                \n",
    "    return results\n",
    "\n",
    "def unwind_descriptions(short_desc):\n",
    "    # get the text from the OrderedDicts in the short descriptions\n",
    "    for i, foo in enumerate(short_desc):\n",
    "        if type(foo) != str:\n",
    "            if type(foo) == list:\n",
    "                for j, bar in enumerate(foo):\n",
    "                    if type(bar) == collections.OrderedDict:\n",
    "                        bar = bar['#text']\n",
    "                        short_desc[i][j] = bar\n",
    "            elif type(foo) == collections.OrderedDict:\n",
    "                foo = foo['#text']\n",
    "                short_desc[i] = foo\n",
    "\n",
    "    # flatten the lists\n",
    "    for i, foo in enumerate(short_desc):\n",
    "        if type(foo) == list:\n",
    "            foo = \" \".join(foo)\n",
    "            short_desc[i] = foo\n",
    "            \n",
    "    return short_desc\n",
    "\n",
    "# function to recursively extract data from XML files\n",
    "def extract_xml(xml_dict, parent_key=\"\", results_dict={}):\n",
    "    # make sure the input is a an ordered dictionary\n",
    "    if isinstance(xml_dict, collections.OrderedDict):\n",
    "        for key1, value1 in xml_dict.items():\n",
    "            # remove unneeded characters from the key\n",
    "            if key1[0] == \"@\" or key1[0] == \"#\":\n",
    "                key1 = key1[1:]\n",
    "            \n",
    "            # FT means FT, we can ignore these\n",
    "            if key1 == \"FT\":\n",
    "                continue\n",
    "            \n",
    "            # add the parent key for clarity\n",
    "            if len(parent_key):\n",
    "                # if the current key is text we will not append it to the parent\n",
    "                if key1 != \"text\":\n",
    "                    new_key = parent_key + \"__\" + key1\n",
    "                else:\n",
    "                    new_key = parent_key\n",
    "            else:\n",
    "                new_key = key1\n",
    "            \n",
    "            # if the value is a string directly add it\n",
    "            if isinstance(value1, str):\n",
    "                # if the key is \"P\" the value is a new paragraph and should be appended\n",
    "                # not overwritten, if the key is \"FT\" it is a font thing and should also be appended\n",
    "                if key1 != \"P\" and key1 != \"FT\":\n",
    "                    # if the key does NOT exist add it\n",
    "                    if new_key not in results_dict:\n",
    "                        results_dict[new_key] = value1\n",
    "                    # else instead of overwriting the data let's make a list of the values\n",
    "                    else:\n",
    "                        if isinstance(results_dict[new_key], list):\n",
    "                            results_dict[new_key].append(value1)\n",
    "                        elif isinstance(results_dict[new_key], str):\n",
    "                            results_dict[new_key] = [results_dict[new_key]]\n",
    "                            results_dict[new_key].append(value1)\n",
    "                else:\n",
    "                    if parent_key in results_dict:\n",
    "                        if isinstance(results_dict[parent_key], list):\n",
    "                            results_dict[parent_key].append(value1)\n",
    "                        elif isinstance(results_dict[parent_key], str):\n",
    "                            listed_vals = [results_dict[parent_key], value1]\n",
    "                            results_dict[parent_key] = listed_vals\n",
    "                    else:\n",
    "                        results_dict[parent_key] = value1\n",
    "            \n",
    "            # else if it is a list loop through and add the items\n",
    "            # note that this will overwrite the previous entries\n",
    "            elif isinstance(value1, list):\n",
    "                item_string = []\n",
    "                for item in value1:\n",
    "                    if isinstance(item, collections.OrderedDict):\n",
    "                        results_dict = extract_xml(item, new_key, results_dict)\n",
    "                    elif isinstance(item, str):\n",
    "                        item_string.append(item)\n",
    "                if len(item_string) > 0:\n",
    "                    if key1 != \"P\":\n",
    "                        results_dict[new_key] = item_string\n",
    "                    else:\n",
    "                        results_dict[parent_key] = item_string\n",
    "                        \n",
    "            # else if the value is an OrderedDict recurse\n",
    "            elif isinstance(value1, collections.OrderedDict):\n",
    "                # handle Ps differently, they are paragraphs and do not need to be recursed into\n",
    "                if key1 != \"P\":\n",
    "                    results_dict = extract_xml(value1, new_key, results_dict)\n",
    "                else:\n",
    "                    # if the key is P and is has text use the text, otherwise recurse as usual\n",
    "                    try:\n",
    "                        results_dict[parent_key] = value1['#text']\n",
    "                    except:\n",
    "                        results_dict = extract_xml(value1, new_key, results_dict)\n",
    "                    \n",
    "    elif isinstance(xml_dict, str):\n",
    "        results_dict[parent_key] = xml_dict\n",
    "    \n",
    "    elif isinstance(xml_dict, list):\n",
    "        pass\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "\n",
    "# list of EU country codes\n",
    "EU_CODES = [\"BE\", \"BG\", \"CZ\", \"DK\", \"DE\", \"EE\", \"IE\", \"EL\", \"ES\", \"FR\", \"HR\", \"IT\", \"CY\", \"LV\", \"LT\", \"LU\", \"HU\", \"MT\", \"NL\", \"AT\", \"PL\", \"PT\", \"RO\", \"SI\", \"SK\", \"FI\", \"SE\", \"UK\"]\n",
    "\n",
    "\n",
    "## Function load_data - \n",
    "## Params -\n",
    "## - files - list of files to load\n",
    "## - language - languages to extract from the XML\n",
    "## - doc_type_filter - if specified function will only return XML documents of the specified type\n",
    "## Returns - \n",
    "## - dataframe of parsed documents\n",
    "def load_data(files, language=\"EN\", doc_type_filter=None):\n",
    "    parsed_xmls = []\n",
    "    \n",
    "    language_tenders = []\n",
    "    all_tenders = []\n",
    "    \n",
    "    # clean the file names\n",
    "    dir_list = []\n",
    "    for file in files:\n",
    "        file_name = file.split(\".\")[0]\n",
    "        file_array = file_name.split(\"/\")\n",
    "        file_name = file_array[1]\n",
    "        split_file_name = file_name.split(\"_\")\n",
    "        # remove the year from the second part of the split file name\n",
    "        split_file_name[1] = split_file_name[1][4:]\n",
    "        dir_name = \"_\".join(split_file_name)\n",
    "        dir_list.append(dir_name)\n",
    "        \n",
    "    # loop through the files\n",
    "    for dir_ in dir_list:\n",
    "        files = os.listdir(os.path.join(data_path, dir_))\n",
    "        date = dir_.split(\"_\")[0]\n",
    "        for file in files:\n",
    "            # read the contents of the file\n",
    "            with io.open(os.path.join(data_path, dir_, file), 'r', encoding=\"utf-8\") as f:\n",
    "                xml = f.read()\n",
    "                parsed_xml = xmltodict.parse(xml)\n",
    "                \n",
    "                if doc_type_filter is not None and parsed_xml['TED_EXPORT']['CODED_DATA_SECTION']['CODIF_DATA']['TD_DOCUMENT_TYPE']['#text'] != doc_type_filter:\n",
    "                    continue\n",
    "                    \n",
    "                parsed_xmls.append(parsed_xml)\n",
    "                \n",
    "                # get some header info\n",
    "                forms_section = parsed_xml['TED_EXPORT']['FORM_SECTION']\n",
    "                notice_data = parsed_xml['TED_EXPORT']['CODED_DATA_SECTION']['NOTICE_DATA']\n",
    "                \n",
    "                header_info = {}\n",
    "                header_info['DATE'] = date\n",
    "                header_info['FILE'] = file\n",
    "                # extract the info from the codified data section\n",
    "                header_info = extract_xml(parsed_xml['TED_EXPORT']['CODED_DATA_SECTION']['CODIF_DATA'], \"\", header_info)\n",
    "                \n",
    "                # extract the info from the notice_data section, except we don't need the URI_LIST\n",
    "                notice_data.pop(\"URI_LIST\")\n",
    "                header_info = extract_xml(notice_data, \"\", header_info)\n",
    "                \n",
    "                if isinstance(notice_data['ORIGINAL_CPV'], list):\n",
    "                    header_info['ORIGINAL_CPV_CODE'] = []\n",
    "                    header_info['ORIGINAL_CPV_TEXT'] = []\n",
    "                    for cpv_info in notice_data['ORIGINAL_CPV']:\n",
    "                        header_info['ORIGINAL_CPV_CODE'].append(cpv_info['@CODE'])\n",
    "                        header_info['ORIGINAL_CPV_TEXT'].append(cpv_info['#text'])\n",
    "                else:\n",
    "                    header_info['ORIGINAL_CPV_CODE'] = notice_data['ORIGINAL_CPV']['@CODE']\n",
    "                    header_info['ORIGINAL_CPV_TEXT'] = notice_data['ORIGINAL_CPV']['#text']\n",
    "\n",
    "                try:\n",
    "                    header_info['REF_NO'] = notice_data['REF_NOTICE']['NO_DOC_OJS']\n",
    "                except:\n",
    "                    header_info['REF_NO'] = \"\"\n",
    "                    \n",
    "                forms = forms_section.keys()\n",
    "                \n",
    "                for form in forms:\n",
    "                    try:\n",
    "                        form_contents = forms_section[form]\n",
    "                            \n",
    "                        if isinstance(form_contents, list):\n",
    "                            for i, form_content in enumerate(form_contents):\n",
    "                                all_tenders.append((header_info, form_content))\n",
    "                                if language is not None and form_content['@LG'] == language:\n",
    "                                    language_tenders.append((header_info, form_content))\n",
    "                        elif isinstance(form_contents, collections.OrderedDict):\n",
    "                            all_tenders.append((header_info, form_contents))\n",
    "                            if language is not None and form_contents['@LG'] == language:\n",
    "                                language_tenders.append((header_info, form_contents))\n",
    "                    except Exception as e:\n",
    "                        print(\"File 1\", file, e)\n",
    "\n",
    "    if language == None:\n",
    "        language_tenders = all_tenders\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    for (header, tender) in language_tenders:\n",
    "        flattened = {}\n",
    "        \n",
    "        # add some fields\n",
    "        for key in header.keys():\n",
    "            flattened[key] = header[key]\n",
    "        \n",
    "        flattened = extract_xml(tender, \"\", flattened)\n",
    "        \n",
    "        parsed_data.append(flattened)\n",
    "\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "        \n",
    "    # try convert Currencies to Euros, some doc types don't have this so it's not a big deal if there's an error\n",
    "    try:\n",
    "        df['VALUE_EUR'] = convert_currencies(df['VALUES_VALUE'].values, df['VALUES_VALUE_CURRENCY'].values)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return_df = pd.DataFrame(columns=USE_COLS)\n",
    "    for col in df.columns:\n",
    "        if col in USE_COLS:\n",
    "            return_df[col] = df[col]\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_files(data_path=\"data\", ftp_path=\"91.250.107.123\", username=\"guest\", password=\"guest\", year=None, month=None, max_files=None, delete_files=True):\n",
    "    new_files = download_files(data_path=data_path, ftp_path=ftp_path, username=username, password=password, year=year, month=month, max_files=max_files, delete_files=delete_files)\n",
    "\n",
    "    df = load_data(new_files)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180124_2018016.tar.gz\n",
      "100% [..........................................................................] 6856557 / 6856557\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180126_2018018.tar.gz\n",
      "100% [..........................................................................] 7067424 / 7067424\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180111_2018007.tar.gz\n",
      "100% [..........................................................................] 6620219 / 6620219\n",
      "Extracting: data/20180124_2018016.tar.gz\n",
      "\n",
      "Extracting: data/20180126_2018018.tar.gz\n",
      "\n",
      "Extracting: data/20180111_2018007.tar.gz\n"
     ]
    }
   ],
   "source": [
    "df = load_new_files(year=\"2018\", month=\"01\", max_files=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA_AUTHORITY_TYPE</th>\n",
       "      <th>AA_AUTHORITY_TYPE__CODE</th>\n",
       "      <th>AC_AWARD_CRIT</th>\n",
       "      <th>AC_AWARD_CRIT__CODE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DS_DATE_DISPATCH</th>\n",
       "      <th>FILE</th>\n",
       "      <th>HEADING</th>\n",
       "      <th>ISO_COUNTRY__VALUE</th>\n",
       "      <th>...</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__ADDRESS</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__BLK_BTX</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__COUNTRY__VALUE</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__E_MAIL</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__ORGANISATION</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__PHONE</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__POSTAL_CODE</th>\n",
       "      <th>FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__TOWN</th>\n",
       "      <th>FD_OTH_NOT__TI_DOC</th>\n",
       "      <th>VERSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>European Institution/Agency or International O...</td>\n",
       "      <td>5</td>\n",
       "      <td>The most economic tender</td>\n",
       "      <td>2</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>20180112</td>\n",
       "      <td>20180105</td>\n",
       "      <td>012813_2018.xml</td>\n",
       "      <td>AGC03</td>\n",
       "      <td>BE</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European Institution/Agency or International O...</td>\n",
       "      <td>5</td>\n",
       "      <td>The most economic tender</td>\n",
       "      <td>2</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>20180112</td>\n",
       "      <td>20180105</td>\n",
       "      <td>012814_2018.xml</td>\n",
       "      <td>AGC03</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>European Institution/Agency or International O...</td>\n",
       "      <td>5</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Z</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>20180112</td>\n",
       "      <td>20180109</td>\n",
       "      <td>012815_2018.xml</td>\n",
       "      <td>BI3EA</td>\n",
       "      <td>LU</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AUTO_FILL_IN_DELIVERY</td>\n",
       "      <td>R2.0.8.S03.E01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Utilities entity</td>\n",
       "      <td>4</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Z</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>20180112</td>\n",
       "      <td>20180108</td>\n",
       "      <td>012816_2018.xml</td>\n",
       "      <td>BR405</td>\n",
       "      <td>KZ</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EBRD - Bozoi Gas Storage Facility (KZ-Astana)</td>\n",
       "      <td>R2.0.8.S03.E01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Utilities entity</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>8</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>20180112</td>\n",
       "      <td>20180108</td>\n",
       "      <td>012817_2018.xml</td>\n",
       "      <td>BR505</td>\n",
       "      <td>HR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EBRD - Croatian Railways Modernisation Project...</td>\n",
       "      <td>R2.0.8.S03.E01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   AA_AUTHORITY_TYPE AA_AUTHORITY_TYPE__CODE  \\\n",
       "0  European Institution/Agency or International O...                       5   \n",
       "1  European Institution/Agency or International O...                       5   \n",
       "2  European Institution/Agency or International O...                       5   \n",
       "3                                   Utilities entity                       4   \n",
       "4                                   Utilities entity                       4   \n",
       "\n",
       "              AC_AWARD_CRIT AC_AWARD_CRIT__CODE  CATEGORY      DATE  \\\n",
       "0  The most economic tender                   2  ORIGINAL  20180112   \n",
       "1  The most economic tender                   2  ORIGINAL  20180112   \n",
       "2             Not specified                   Z  ORIGINAL  20180112   \n",
       "3             Not specified                   Z  ORIGINAL  20180112   \n",
       "4                     Other                   8  ORIGINAL  20180112   \n",
       "\n",
       "  DS_DATE_DISPATCH             FILE HEADING ISO_COUNTRY__VALUE  \\\n",
       "0         20180105  012813_2018.xml   AGC03                 BE   \n",
       "1         20180105  012814_2018.xml   AGC03                 SE   \n",
       "2         20180109  012815_2018.xml   BI3EA                 LU   \n",
       "3         20180108  012816_2018.xml   BR405                 KZ   \n",
       "4         20180108  012817_2018.xml   BR505                 HR   \n",
       "\n",
       "        ...       FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__ADDRESS  \\\n",
       "0       ...                                                      NaN    \n",
       "1       ...                                                      NaN    \n",
       "2       ...                                                      NaN    \n",
       "3       ...                                                      NaN    \n",
       "4       ...                                                      NaN    \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__BLK_BTX  \\\n",
       "0                                                NaN    \n",
       "1                                                NaN    \n",
       "2                                                NaN    \n",
       "3                                                NaN    \n",
       "4                                                NaN    \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__COUNTRY__VALUE  \\\n",
       "0                                                NaN           \n",
       "1                                                NaN           \n",
       "2                                                NaN           \n",
       "3                                                NaN           \n",
       "4                                                NaN           \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__E_MAIL  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__ORGANISATION  \\\n",
       "0                                                NaN         \n",
       "1                                                NaN         \n",
       "2                                                NaN         \n",
       "3                                                NaN         \n",
       "4                                                NaN         \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__PHONE  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__POSTAL_CODE  \\\n",
       "0                                                NaN        \n",
       "1                                                NaN        \n",
       "2                                                NaN        \n",
       "3                                                NaN        \n",
       "4                                                NaN        \n",
       "\n",
       "  FD_OTH_NOT__STI_DOC__P__ADDRESS_NOT_STRUCT__TOWN  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "                                  FD_OTH_NOT__TI_DOC         VERSION  \n",
       "0                                                NaN             NaN  \n",
       "1                                                NaN             NaN  \n",
       "2                              AUTO_FILL_IN_DELIVERY  R2.0.8.S03.E01  \n",
       "3      EBRD - Bozoi Gas Storage Facility (KZ-Astana)  R2.0.8.S03.E01  \n",
       "4  EBRD - Croatian Railways Modernisation Project...  R2.0.8.S03.E01  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
