{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions which will download the tar.gz files, extract them, log the files downloaded so as not to redownload them in the future. Then another function is passed the names of the files successfully extracted and that function loops through the files, loads and parses them - also logging the names of the .xml files successfully processed.\n",
    "\n",
    "Note that this depends on the scheme TED uses to name the folders inside of the .tar.gz files not changing (the folder has a different name than the containing tarball.) There are a couple possible solutions to this I can think of:\n",
    " * Download the files to a temp directory and move them after they have been processed.\n",
    " * Delete the .xml files from disk after having been processed (or they can be put into a NoSQL database) - they can always be redownloaded if necessary.\n",
    " \n",
    "The next steps will be determing which of the 1000+ columns to keep and which to drop and putting the data into a database (we need to choose a database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import datetime\n",
    "import os\n",
    "import tarfile\n",
    "import wget\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "import io\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"logs\"\n",
    "TMP_PATH = \"tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function download_files:\n",
    "# FTPs to ftp_path, gets list of files in the directory for the current year and month (note that this may cause\n",
    "# problems on the first day of the month downloading the files from the last day of the previous month); makes a list \n",
    "# of files to be download. Then downloads the files with wget (ftp was not downloading the entire file); unzips the \n",
    "# tarballs and then deletes the tar.gz file.\n",
    "# \n",
    "# Params:\n",
    "# - data_path -> path to download data to\n",
    "# - ftp_path -> URI for FTP\n",
    "# - username, password -> username and password for FTP login\n",
    "# - year, month -> year and month to download data for, if None will use current month and year\n",
    "# - max_files -> max number of files to download, useful for debuggin\n",
    "# - delete_files -> whether to delete the files that have been successfully extracted\n",
    "#\n",
    "# Returns:\n",
    "# - list of files downloaded and successfully extracted\n",
    "#\n",
    "# Note that sometimes using the URL throws an error, in this case use the IP address: 91.250.107.123\n",
    "\n",
    "def download_files(data_path=\"data\", ftp_path=\"91.250.107.123\", username=\"guest\", password=\"guest\", year=None, month=None, max_files=None, delete_files=True):\n",
    "    # open the log of downloaded files so we know what NOT to download\n",
    "    downloaded_files = pd.read_csv(os.path.join(LOG_PATH, \"download_logs.txt\"), header=None).values\n",
    "    \n",
    "    ## USE FTP TO GET THE LIST OF FILES TO DOWNLOAD\n",
    "    with FTP(ftp_path, user=username, passwd=password) as ftp:\n",
    "        # create the directory name for the current month and year\n",
    "        # we may want to do this for yesterday \n",
    "        now = datetime.datetime.now()\n",
    "        if year is None:\n",
    "            year = str(now.year)\n",
    "        if month is None:\n",
    "            month = datetime.datetime.now().strftime('%m')\n",
    "\n",
    "        # go to that directory and get the files in it\n",
    "        ftp.cwd('daily-packages/' + year + \"/\" + month) \n",
    "        dir_list = ftp.nlst() \n",
    "        files_to_download = []\n",
    "\n",
    "        # loop through the files\n",
    "        for file in dir_list:\n",
    "            # if the file is not in the logs\n",
    "            if file not in downloaded_files:\n",
    "                # download the file with wget since ftplib seems to only download a small part of the file\n",
    "                file_path = \"ftp://\"+username+\":\"+password+\"@\" + ftp_path + \"/daily-packages/\" + year + \"/\" + month + \"/\" + file\n",
    "                files_to_download.append(file_path)\n",
    "    \n",
    "    # delete the downloaded files\n",
    "    del(downloaded_files)\n",
    "    \n",
    "    if max_files is not None:\n",
    "        files_to_download = files_to_download[:max_files]\n",
    "    \n",
    "    # download the files with wget so we can download the entire file without errors            \n",
    "    downloaded_files = []\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            print(\"\\nDownloading\", file)\n",
    "            d_file = wget.download(file, data_path)\n",
    "            downloaded_files.append(d_file)\n",
    "        except:\n",
    "            print(\"Error downloading\", file)\n",
    "    \n",
    "    extracted_files = []\n",
    "    # extract the tarballs\n",
    "    for file in downloaded_files:\n",
    "        print(\"\\nExtracting:\", file)\n",
    "        try:\n",
    "            if (file.endswith(\"tar.gz\")):\n",
    "                tar = tarfile.open(file, \"r:gz\")\n",
    "                tar.extractall(data_path)\n",
    "                tar.close()\n",
    "            elif (file.endswith(\"tar\")):\n",
    "                tar = tarfile.open(file, \"r:\")\n",
    "                tar.extractall()\n",
    "                tar.close()\n",
    "            \n",
    "            extracted_files.append(file)\n",
    "            if delete_files:\n",
    "                # if everything was properly extracted we can delete the file\n",
    "                os.remove(file)\n",
    "            \n",
    "            # try to open the file in append mode, if it doesn't work create a new file\n",
    "            try:\n",
    "                f = open(os.path.join(LOG_PATH, \"download_logs.txt\"),\"a\")\n",
    "            except:\n",
    "                f = open(os.path.join(LOG_PATH, \"download_logs.txt\"),\"w+\")\n",
    "            file_name = file.split(\"/\")[1]\n",
    "            f.write(file_name + \"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "        except:\n",
    "            print(\"Error extracting\", file)\n",
    "\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all currencies to EUR\n",
    "def convert_currencies(values, currencies):\n",
    "    url = \"https://api.exchangeratesapi.io/latest\"\n",
    "    content = urllib.request.urlopen(url).read()\n",
    "    exchange_rates = json.loads(content.decode())\n",
    "    results = []\n",
    "    \n",
    "    for value, currency in zip(values, currencies):\n",
    "        if currency == \"EUR\":\n",
    "            results.append(value)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                exchange_rate = exchange_rates['rates'][currency]\n",
    "                converted_value = float(value) / exchange_rate\n",
    "                results.append(converted_value)\n",
    "            # if we don't have a rate for the currency use NaN\n",
    "            except:\n",
    "                results.append(np.nan)\n",
    "                \n",
    "    return results\n",
    "\n",
    "def unwind_descriptions(short_desc):\n",
    "    # get the text from the OrderedDicts in the short descriptions\n",
    "    for i, foo in enumerate(short_desc):\n",
    "        if type(foo) != str:\n",
    "            if type(foo) == list:\n",
    "                for j, bar in enumerate(foo):\n",
    "                    if type(bar) == collections.OrderedDict:\n",
    "                        bar = bar['#text']\n",
    "                        short_desc[i][j] = bar\n",
    "            elif type(foo) == collections.OrderedDict:\n",
    "                foo = foo['#text']\n",
    "                short_desc[i] = foo\n",
    "\n",
    "    # flatten the lists\n",
    "    for i, foo in enumerate(short_desc):\n",
    "        if type(foo) == list:\n",
    "            foo = \" \".join(foo)\n",
    "            short_desc[i] = foo\n",
    "            \n",
    "    return short_desc\n",
    "\n",
    "# function to recursively extract data from XML files\n",
    "def extract_xml(xml_dict, parent_key=\"\", results_dict={}):\n",
    "    # make sure the input is a an ordered dictionary\n",
    "    if isinstance(xml_dict, collections.OrderedDict):\n",
    "        for key1, value1 in xml_dict.items():\n",
    "            # remove unneeded characters from the key\n",
    "            if key1[0] == \"@\" or key1[0] == \"#\":\n",
    "                key1 = key1[1:]\n",
    "                \n",
    "            # add the parent key for clarity\n",
    "            if len(parent_key):\n",
    "                # if the current key is text we will not append it to the parent\n",
    "                if key1 != \"text\":\n",
    "                    new_key = parent_key + \"_\" + key1\n",
    "                else:\n",
    "                    new_key = parent_key\n",
    "            else:\n",
    "                new_key = key1\n",
    "            \n",
    "            # if the value is a string directly add it\n",
    "            if isinstance(value1, str):\n",
    "                # if the key is \"P\" the value is a new paragraph and should be appended not overwritten\n",
    "                if key1 != \"P\":\n",
    "                    # if the key does NOT exist add it\n",
    "                    if new_key not in results_dict:\n",
    "                        results_dict[new_key] = value1\n",
    "                    # else instead of overwriting the data let's make a list of the values\n",
    "                    else:\n",
    "                        if isinstance(results_dict[new_key], list):\n",
    "                            results_dict[new_key].append(value1)\n",
    "                        elif isinstance(results_dict[new_key], str):\n",
    "                            results_dict[new_key] = [results_dict[new_key]]\n",
    "                            results_dict[new_key].append(value1)\n",
    "                else:\n",
    "                    if parent_key in results_dict:\n",
    "                        results_dict[parent_key] += \" \" + value1\n",
    "                    else:\n",
    "                        results_dict[parent_key] = value1\n",
    "            \n",
    "            # else if it is a list loop through and add the items\n",
    "            # note that this will overwrite the previous entries\n",
    "            elif isinstance(value1, list):\n",
    "                for item in value1:\n",
    "                    results_dict = extract_xml(item, new_key, results_dict)\n",
    "                    \n",
    "            # else if the value is an OrderedDict recurse\n",
    "            elif isinstance(value1, collections.OrderedDict):\n",
    "                results_dict = extract_xml(value1, new_key, results_dict)\n",
    "                \n",
    "    elif isinstance(xml_dict, str):\n",
    "        results_dict[parent_key] = xml_dict\n",
    "    \n",
    "    elif isinstance(xml_dict, list):\n",
    "        pass\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "\n",
    "# list of EU country codes\n",
    "EU_CODES = [\"BE\", \"BG\", \"CZ\", \"DK\", \"DE\", \"EE\", \"IE\", \"EL\", \"ES\", \"FR\", \"HR\", \"IT\", \"CY\", \"LV\", \"LT\", \"LU\", \"HU\", \"MT\", \"NL\", \"AT\", \"PL\", \"PT\", \"RO\", \"SI\", \"SK\", \"FI\", \"SE\", \"UK\"]\n",
    "\n",
    "def load_data(files, language=\"EN\", max_dirs=None):\n",
    "    parsed_xmls = []\n",
    "    \n",
    "    language_tenders = []\n",
    "    all_tenders = []\n",
    "    \n",
    "    # clean the file names\n",
    "    dir_list = []\n",
    "    for file in files:\n",
    "        file_name = file.split(\".\")[0]\n",
    "        file_array = file_name.split(\"/\")\n",
    "        file_name = file_array[1]\n",
    "        split_file_name = file_name.split(\"_\")\n",
    "        # remove the year from the second part of the split file name\n",
    "        split_file_name[1] = split_file_name[1][4:]\n",
    "        dir_name = \"_\".join(split_file_name)\n",
    "        dir_list.append(dir_name)\n",
    "        \n",
    "    # loop through the files\n",
    "    for dir_ in dir_list:\n",
    "        files = os.listdir(os.path.join(data_path, dir_))\n",
    "        date = dir_.split(\"_\")[0]\n",
    "        for file in files:\n",
    "            # read the contents of the file\n",
    "            with io.open(os.path.join(data_path, dir_, file), 'r', encoding=\"utf-8\") as f:\n",
    "                xml = f.read()\n",
    "                parsed_xml = xmltodict.parse(xml)\n",
    "                parsed_xmls.append(parsed_xml)\n",
    "\n",
    "                # get some header info\n",
    "                forms_section = parsed_xml['TED_EXPORT']['FORM_SECTION']\n",
    "                notice_data = parsed_xml['TED_EXPORT']['CODED_DATA_SECTION']['NOTICE_DATA']\n",
    "\n",
    "                header_info = {}\n",
    "                header_info['DATE'] =  parsed_xml['TED_EXPORT']['CODED_DATA_SECTION']['REF_OJS']['DATE_PUB']\n",
    "                header_info['FILE'] = file\n",
    "                # extract the info from the codified data section\n",
    "                header_info = extract_xml(parsed_xml['TED_EXPORT']['CODED_DATA_SECTION']['CODIF_DATA'], \"\", header_info)\n",
    "\n",
    "                # extract the info from the notice_data section, except we don't need the URI_LIST\n",
    "                notice_data.pop(\"URI_LIST\")\n",
    "                header_info = extract_xml(notice_data, \"\", header_info)\n",
    "\n",
    "                if isinstance(notice_data['ORIGINAL_CPV'], list):\n",
    "                    header_info['ORIGINAL_CPV_CODE'] = []\n",
    "                    header_info['ORIGINAL_CPV_TEXT'] = []\n",
    "                    for cpv_info in notice_data['ORIGINAL_CPV']:\n",
    "                        header_info['ORIGINAL_CPV_CODE'].append(cpv_info['@CODE'])\n",
    "                        header_info['ORIGINAL_CPV_TEXT'].append(cpv_info['#text'])\n",
    "                else:\n",
    "                    header_info['ORIGINAL_CPV_CODE'] = notice_data['ORIGINAL_CPV']['@CODE']\n",
    "                    header_info['ORIGINAL_CPV_TEXT'] = notice_data['ORIGINAL_CPV']['#text']\n",
    "\n",
    "                try:\n",
    "                    header_info['VALUE'] = notice_data['VALUES']['VALUE']['#text']\n",
    "                    header_info['VALUE_CURR'] = notice_data['VALUES']['VALUE']['@CURRENCY']\n",
    "                    header_info['REF_NO'] = notice_data['REF_NOTICE']['NO_DOC_OJS']\n",
    "                except:\n",
    "                    header_info['VALUE'] = \"\"\n",
    "                    header_info['VALUE_CURR'] = \"\"\n",
    "                    header_info['REF_NO'] = \"\"\n",
    "\n",
    "                forms = forms_section.keys()\n",
    "\n",
    "                for form in forms:\n",
    "                    try:\n",
    "                        form_contents = forms_section[form]\n",
    "\n",
    "                        if isinstance(form_contents, list):\n",
    "                            for i, form_content in enumerate(form_contents):\n",
    "                                all_tenders.append((header_info, form_content))\n",
    "                                if language is not None and form_content['@LG'] == language:\n",
    "                                    language_tenders.append((header_info, form_content))\n",
    "                        elif isinstance(form_contents, collections.OrderedDict):\n",
    "                            all_tenders.append((header_info, form_contents))\n",
    "                            if language is not None and form_contents['@LG'] == language:\n",
    "                                language_tenders.append((header_info, form_contents))\n",
    "                    except Exception as e:\n",
    "                        print(\"File 1\", file, e)\n",
    "\n",
    "    if language == None:\n",
    "        language_tenders = all_tenders\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    for (header, tender) in language_tenders:\n",
    "        flattened = {}\n",
    "        \n",
    "        # add some fields\n",
    "        for key in header.keys():\n",
    "            flattened[key] = header[key]\n",
    "        \n",
    "        flattened = extract_xml(tender, \"\", flattened)\n",
    "        parsed_data.append(flattened)\n",
    "        \n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    # convert Currencies to Euros\n",
    "    df['VALUE_EUR'] = convert_currencies(df['VALUE'].values, df['VALUE_CURR'].values)\n",
    "    \n",
    "    # log the files successfully processed\n",
    "    try:\n",
    "        f = open(os.path.join(LOG_PATH, \"processed_logs.txt\"),\"a\")\n",
    "    except:\n",
    "        f = open(os.path.join(LOG_PATH, \"processed_logs.txt\"),\"w+\")\n",
    "    \n",
    "    for file_name in df['FILE'].unique():\n",
    "        f.write(file_name + \"\\n\")\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_files(data_path=\"data\", ftp_path=\"91.250.107.123\", username=\"guest\", password=\"guest\", year=None, month=None, max_files=None, delete_files=True):\n",
    "    new_files = download_files(data_path=data_path, ftp_path=ftp_path, username=username, password=password, year=year, month=month, max_files=max_files, delete_files=delete_files)\n",
    "    \n",
    "    df = load_data(new_files)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180113_2018009.tar.gz\n",
      "100% [..........................................................................] 6449487 / 6449487\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180127_2018019.tar.gz\n",
      "100% [..........................................................................] 7330376 / 7330376\n",
      "Downloading ftp://guest:guest@91.250.107.123/daily-packages/2018/01/20180103_2018001.tar.gz\n",
      "100% [..........................................................................] 6444453 / 6444453\n",
      "Extracting: data/20180113_2018009.tar.gz\n",
      "\n",
      "Extracting: data/20180127_2018019.tar.gz\n",
      "\n",
      "Extracting: data/20180103_2018001.tar.gz\n"
     ]
    }
   ],
   "source": [
    "df = load_new_files(year=\"2018\", month=\"01\", max_files=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
